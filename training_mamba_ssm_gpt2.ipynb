{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c1c48c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mamba-ssm in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (2.2.6.post3)\n",
      "Requirement already satisfied: torch in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from mamba-ssm) (2.9.0)\n",
      "Requirement already satisfied: triton in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from mamba-ssm) (3.5.0)\n",
      "Requirement already satisfied: ninja in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from mamba-ssm) (1.13.0)\n",
      "Requirement already satisfied: einops in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from mamba-ssm) (0.8.1)\n",
      "Requirement already satisfied: transformers in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from mamba-ssm) (4.57.1)\n",
      "Requirement already satisfied: packaging in /home/mahesh/.local/lib/python3.12/site-packages (from mamba-ssm) (24.2)\n",
      "Requirement already satisfied: setuptools>=61.0.0 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from mamba-ssm) (80.9.0)\n",
      "Requirement already satisfied: filelock in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from torch->mamba-ssm) (1.13.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from sympy>=1.13.3->torch->mamba-ssm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from jinja2->torch->mamba-ssm) (3.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from transformers->mamba-ssm) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from transformers->mamba-ssm) (2.3.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from transformers->mamba-ssm) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from transformers->mamba-ssm) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from transformers->mamba-ssm) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from transformers->mamba-ssm) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from transformers->mamba-ssm) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from transformers->mamba-ssm) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->mamba-ssm) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from requests->transformers->mamba-ssm) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from requests->transformers->mamba-ssm) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from requests->transformers->mamba-ssm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages (from requests->transformers->mamba-ssm) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mamba-ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba50231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mamba_ssm\n",
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "from mamba_ssm import Mamba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301e8477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.6.post3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mamba_ssm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11205ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model\n",
    "dim = 128\n",
    "n_layers = 2\n",
    "Batch_Size = 16\n",
    "lr = 1e-5\n",
    "epochs=5\n",
    "\n",
    "# setting up the device \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498bcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MambaSSM_model = Mamba2\n",
    "# (\n",
    "#     # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "#     d_model=dim, # Model dimension d_model\n",
    "#     d_state=128, # SSM state expansion factor\n",
    "#     d_conv=4,   # Local convolution width\n",
    "#     conv_init=None,\n",
    "#     expand=2,  # Block expansion factor\n",
    "#     headdim=64,\n",
    "#     d_ssm=None,  # If not None, we only apply SSM on this many dimensions, the rest uses gated MLP\n",
    "#     ngroups=1,\n",
    "#     A_init_range=(1, 16),\n",
    "#     D_has_hdim=False,\n",
    "#     rmsnorm=True,\n",
    "#     norm_before_gate=False,\n",
    "#     dt_min=0.001,\n",
    "#     dt_max=0.1,\n",
    "#     dt_init_floor=1e-4,\n",
    "#     dt_limit=(0.0, float(\"inf\")),\n",
    "#     bias=False,\n",
    "#     conv_bias=True,\n",
    "#     # Fused kernel and sharding options\n",
    "#     chunk_size=256,\n",
    "#     use_mem_eff_path=True,\n",
    "#     layer_idx=None,  # Absorb kwarg for general module\n",
    "#     process_group=None,\n",
    "#     sequence_parallel=True,\n",
    "# ).to(device)\n",
    "\n",
    "from mamba_ssm import Mamba2\n",
    "import torch.nn as nn\n",
    "\n",
    "Mamba_block = Mamba2(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    "    rmsnorm=True,\n",
    "    ngroups = 2,\n",
    "    use_mem_eff_path=True,\n",
    "    chunk_size=256,\n",
    "    layer_idx=None,  # Absorb kwarg for general module\n",
    ").to(device=device)\n",
    "\n",
    "class MambaStack(nn.Module):\n",
    "    def __init__(self,model,n_layers=12,**kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            model for _ in range(n_layers)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "MambaSSMmodel = MambaStack(Mamba_block,n_layers = n_layers).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22f5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.models.config_mamba import MambaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de948ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchtext\n",
      "Version: 0.5.0\n",
      "Summary: Text utilities and datasets for PyTorch\n",
      "Home-page: https://github.com/pytorch/text\n",
      "Author: PyTorch core devs and James Bradbury\n",
      "Author-email: jekbradbury@gmail.com\n",
      "License: BSD\n",
      "Location: /home/mahesh/miniconda3/envs/mcd_env/lib/python3.12/site-packages\n",
      "Requires: numpy, requests, sentencepiece, six, torch, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be795890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:06, 18089.73lines/s]\n",
      "120000lines [00:12, 9727.03lines/s] \n",
      "7600lines [00:00, 10933.65lines/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading a simple Dataset AG News classification Dataset \n",
    "import torchtext\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "# train_iter = AG_NEWS(split='train')\n",
    "train_dataset,test_dataset = AG_NEWS(root='/home/mahesh/sharath_MTP/Metro_Data_1',\n",
    "                                     ngrams = 3,\n",
    "                                     vocab = None,\n",
    "                                     include_unk = False\n",
    "                                    )\n",
    "# test_iter = AG_NEWS(split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfdfc421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:29, 4054.38lines/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# making a traindata iterator for making the vocab \n",
    "train_iter = list(train_dataset) \n",
    "\n",
    "def get_token(data_iter):\n",
    "    for _,tokens in data_iter:\n",
    "        yield tokens\n",
    "        \n",
    "vocab = build_vocab_from_iterator(get_token(train_iter))\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label)  \n",
    "        # adding a list comprehension to convert tokens to indices so the vocab object is not called directly\n",
    "        # and remains callable \n",
    "        processed_text = torch.tensor([vocab[token] for token in _text], dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list, text_list, offsets\n",
    "\n",
    "# Making the testing and training dataloaders \n",
    "train_Dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=Batch_Size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "test_Dataloader = DataLoader(test_dataset,\n",
    "                       batch_size=Batch_Size,\n",
    "                       shuffle=True,\n",
    "                       collate_fn=collate_fn)\n",
    "\n",
    "# train_iter = AG_NEWS(root='/home/mahesh/sharath_MTP/Metro_Data_1',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3acc850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 2, 2, 3, 2, 1, 2, 2, 3, 3, 3, 2, 1, 3, 1])\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Replacing the vocab(_text) with a list comprehension [vocab[token] for token in _text], which maps each token to its index.\n",
    "# This will avoid the 'Vocab' object is not callable error.\n",
    "for labels,text,_ in train_Dataloader:\n",
    "    print(labels)\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b552621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15220829\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd2315f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show transformers\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM,GPT2Config,GPT2LMHeadModel,GPT2Model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size = vocab_size,\n",
    "    n_embd=dim,\n",
    "    n_layer=n_layers,\n",
    "    n_head=8,\n",
    "    layer_norm_epsilon=1e-5,\n",
    "    eos_token_id=vocab_size,\n",
    ")\n",
    "\n",
    "GPT2_Model = GPT2LMHeadModel(config).to(device=device)\n",
    "# Defining the tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18b77d80",
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx,(labels,text,offsets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_Dataloader):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     inputs = \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     labels = text.to(device=device)\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# GPT2 Model Training\u001b[39;00m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim \n",
    "from torch.optim import AdamW\n",
    "import os \n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "optimizer = optim.AdamW(GPT2_Model.parameters(),lr = lr)\n",
    "optimizer_1 = optim.AdamW(MambaSSMmodel.parameters(),lr = lr)\n",
    "\n",
    "# # i --- IGNORE ---\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "total_steps = len(train_Dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "scheduler_1 = get_linear_schedule_with_warmup(optimizer_1,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "# # Training Loop\n",
    "for epoch in range(epochs): \n",
    "    GPT2_Model.train()\n",
    "    MambaSSMmodel.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx,(labels,text,offsets) in enumerate(train_Dataloader):\n",
    "        inputs = text.to(device=device)\n",
    "        labels = text.to(device=device)\n",
    "        \n",
    "        # GPT2 Model Training\n",
    "        optimizer.zero_grad()\n",
    "        outputs = GPT2_Model(inputs,labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # MambaSSM Model Training\n",
    "        optimizer_1.zero_grad()\n",
    "        embeddings = nn.Embedding(vocab_size,dim).to(device=device)\n",
    "        input_embeddings = embeddings(inputs)\n",
    "        mamba_outputs = MambaSSMmodel(input_embeddings)\n",
    "        # Using a linear layer to project the outputs to vocab size for computing loss\n",
    "        linear_layer = nn.Linear(dim,vocab_size).to(device=device)\n",
    "        logits = linear_layer(mamba_outputs)\n",
    "        loss_1 = nn.CrossEntropyLoss()(logits.view(-1,vocab_size),labels.view(-1))\n",
    "        loss_1.backward()\n",
    "        optimizer_1.step()\n",
    "        scheduler_1.step()\n",
    "        \n",
    "        total_loss += loss.item() + loss_1.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_Dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")        \n",
    "    \n",
    "# # Saving the models\n",
    "torch.save(GPT2_Model.state_dict(),\"GPT2_MambaSSM_AGNews.pth\")\n",
    "torch.save(MambaSSMmodel.state_dict(),\"MambaSSM_AGNews.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920ca0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
